{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wikipedia\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(y) for y in np.arange(2004, 2023, 1)]\n",
    "weeks = [int(v) for v in np.linspace(1, 52, 12)]\n",
    "mois = [\"Janvier\", \"Fevrier\", \"Mars\", \"Avril\", \"Mai\", \"Juin\", \"Juillet\", \"Aout\", \"Septembre\", \"Octobre\", \"Novembre\", \"Decembre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:27<00:00,  4.61s/it]\n"
     ]
    }
   ],
   "source": [
    "dfs_chart = {y : {} for y in years}\n",
    "data = []\n",
    "\n",
    "for y in tqdm(years):\n",
    "    for k, w in enumerate(weeks):\n",
    "        url = f\"https://acharts.co/france_singles_top_100/{y}/{w}\"\n",
    "        rq = get(url)\n",
    "        m = mois[k]\n",
    "        if rq.ok:\n",
    "            soup = BeautifulSoup(rq.text)\n",
    "\n",
    "            chart = soup.find(\"table\", {\"id\" : \"ChartTable\"})\n",
    "            trs = chart.findAll(\"tr\")[1:]\n",
    "\n",
    "            for tr in trs:\n",
    "                music_name = tr.find(\"span\", {\"itemprop\" : \"name\"}).text.lower()\n",
    "                rank = tr.find(\"span\", {\"itemprop\" : \"position\"}).text.lower()\n",
    "                artist_name = tr.find(\"span\", {\"itemprop\" : \"byArtist\"}).text[2:-1].lower()\n",
    "                data.append([m, y, str(rank), str(artist_name), str(music_name)])\n",
    "\n",
    "charts = pd.DataFrame(data, columns=[\"Mois\", \"Annee\", \"Rank\", \"Artist\", \"Music\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "charts.to_csv(\"../etapes/1 - scrapping chart/charts.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanning chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "charts = pd.read_csv(\"../etapes/1 - scrapping chart/charts.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    (\"Djadja\", \"Djadja et Dinaz\"),\n",
    "    (\"Lorie\", \"Lorie Pester\"),\n",
    "    (\"-m-\", \"Matthieu Chedid\"),\n",
    "    (\"Priscilla\", \"Priscilla Betti\"),\n",
    "    (\"I Am\", \"IAM\"),\n",
    "    (\"Sofiane\", \"Sofiane Zermani\"),\n",
    "    (\"Justice\", \"Justice (groupe)\"),\n",
    "    (\"Soma Riba\", \"Collectif Métissé\"),\n",
    "    (\"Fresh\", \"Fresh la Peufra\"),\n",
    "    (\"Rosalia\", \"Rosalía\"),\n",
    "    (\"Italo Brothers\", \"ItaloBrothers\"),\n",
    "    (\"Far\\*east Movement\", \"Far East Movement\"),\n",
    "    (\"Odyssey\", \"Odyssey (groupe)\"),\n",
    "    (\"1789\", \"1789 : Les Amants de la Bastille\"),\n",
    "    (\"Clemence\", \"Clémence Saint-Preux\"),\n",
    "    (\"Rose\", \"Rose (chanteuse)\"),\n",
    "    (\"Laeti\", \"Laetitia Kerfa\"),\n",
    "    (\"La Troupe\", \"Mozart, l'opéra rock\"),\n",
    "    (\"Victoria\", \"Victoria Sio\"),\n",
    "    (\"Christine And The Queens\", \"Redcar (artiste)\"),\n",
    "    (\"Earth and Wind And Fire\", \"Earth, Wind And Fire\"),\n",
    "    (\"Jean Roch\", \"Jean-Roch\"),\n",
    "    (\"Keen V\", \"Keen'V\"),\n",
    "    (\"Dinor\", \"Dinor RDT\"),\n",
    "    (\"Koba La D\", \"Koba LaD\"),\n",
    "    (\"scotts\", \"Travis Scott\"),\n",
    "    (\"louane emera\", \"louane\"),\n",
    "    (\"maitre gims\", \"gims\"),\n",
    "    (\"p!nk\", \"pink\"),\n",
    "    (\"Eva\", \"Eva Queen\"),\n",
    "    (\"dj tiësto\", \"tiesto\"),\n",
    "    (\"Karol\", \"Karol G\"),\n",
    "    (\"Khaled\", \"DJ Khaled\"),\n",
    "    (\"Black Eyed Peas\", \"The Black Eyed Peas\"),\n",
    "    (\"L.E.J\", \"LEJ\"),\n",
    "    (\"Disiz la peste\", \"Disiz\"),\n",
    "    (\"k'maro\", \"k. maro\"),\n",
    "    (\"shin sekaï\", \"The Shin Sekaï\"),\n",
    "    (\"the niro\", \"niro\"),\n",
    "    (\"r.i.o.\", \"rio\"),\n",
    "    (\"zayn\", \"zayn malik\"),\n",
    "    (\"dimitri vegas\", \"dimitri vegas & like mike\"),\n",
    "    (\"cauet\", \"Sébastien Cauet\"),\n",
    "    (\"do\", \"the do\"),\n",
    "    (\"les filles\", \"Aurélie Konaté\")\n",
    "]\n",
    "\n",
    "replace_words = [\n",
    "    (\"\\x9c\", \"oe\"),\n",
    "    (\"œ\", \"oe\"),\n",
    "    (\"$\", \"S\"),\n",
    "]\n",
    "\n",
    "charts[\"Artist\"] = charts[\"Artist\"].str.split(\" x \", regex=False).str[0]\n",
    "charts[\"Artist\"] = charts[\"Artist\"].str.split(\" - \", regex=False).str[-1]\n",
    "charts[\"Artist\"] = charts[\"Artist\"].str.split(\" \\+ \", regex=False).str[0]\n",
    "\n",
    "charts[\"Artist\"] = charts[\"Artist\"].apply(lambda x: \"star academy\" if \"star academy\" in x else x)\n",
    "\n",
    "for n1, n2 in names:\n",
    "    charts[\"Artist\"] = charts[\"Artist\"].apply(lambda x: n2.lower() if x.lower() == n1.lower() else x.lower())\n",
    "\n",
    "for n1, n2 in replace_words:\n",
    "    charts[\"Artist\"] = charts[\"Artist\"].str.replace(n1.lower(), n2.lower(), regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "charts.to_csv(\"../etapes/2 - cleanning chart/charts.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping artist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "charts = pd.read_csv(\"../etapes/2 - cleanning chart/charts.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_title_in_wikipedia(title, pourcentage=0.3):\n",
    "    words = [\"(chanteur)\", \"(chanteuse)\", \"(groupe)\", \"(rappeur)\", \"(rappeuse)\", \"(musicien)\", \"(chanteur français)\", \"(france)\", \"(producteur)\", \"(artiste)\", \"(groupe de musique)\"]\n",
    "\n",
    "    wikipedia.set_lang(\"fr\")\n",
    "    results = wikipedia.search(title, results=10)\n",
    "    distance = []\n",
    "    if len(results) > 0:\n",
    "        for element in results:\n",
    "            if any((w in element.lower()) and (edit_distance(element.lower().split(\" (\")[0].strip(), title.lower().strip())/len(title) < pourcentage) for w in words):\n",
    "                return element\n",
    "\n",
    "            distance.append(edit_distance(title.lower().strip(), element.lower().strip()))\n",
    "\n",
    "        return results[np.argmin(distance)] if min(distance)/len(title) < pourcentage else np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_birth(title):\n",
    "    cols = [\"Naissance\", \"Pays d'origine\", \"Origine\", \"Nationalité\", \"Pays\", \"Summary\"]\n",
    "    nats = [\"franco\", \"français\", \"belge\", \"canadien\", \"libanais\", \"réunionnais\"]\n",
    "    dic = {w : np.NaN for w in cols}\n",
    "\n",
    "    if title is np.NaN:\n",
    "        return dic\n",
    "\n",
    "    url = f\"https://fr.wikipedia.org/wiki/{title}\"\n",
    "    rq = get(url)\n",
    "\n",
    "    if not rq.ok:\n",
    "        return dic\n",
    "    \n",
    "    soup = BeautifulSoup(rq.text)\n",
    "    tables = soup.findAll(\"table\")\n",
    "\n",
    "    for table in tables:\n",
    "        trs = table.findAll(\"tr\")\n",
    "\n",
    "        for tr in trs:\n",
    "            th = tr.find(\"th\")\n",
    "\n",
    "            if th is not None:\n",
    "                for w in cols:\n",
    "                    if w in th.text:\n",
    "                        td = tr.find(\"td\")\n",
    "                        if td is not None:\n",
    "                            dic[w] = td.text.strip().lower()\n",
    "\n",
    "    wikipedia.set_lang(\"fr\")\n",
    "    try:\n",
    "        summary = wikipedia.summary(title, sentences=1)\n",
    "        dic[\"Summary\"] = summary.lower().strip()\n",
    "\n",
    "        if dic[\"Nationalité\"] is np.NaN:\n",
    "            for w in nats:\n",
    "                if w in summary:\n",
    "                    dic[\"Nationalité\"] = w.lower().strip()\n",
    "                    return dic\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupére tous les artistes uniques\n",
    "artist = pd.DataFrame(charts[\"Artist\"].unique(), columns=[\"Artist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Artist_wiki\"] = artist[\"Artist\"].apply(lambda x : find_title_in_wikipedia(x, 0.2)) #Trouve les pages wikipedia de chaque artistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "birth_dic = artist[\"Artist_wiki\"].apply(wiki_birth) #Cherche les infos de naissance sur les pages wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforme les infos trouver sur wikipédia en dataframe\n",
    "dfs_birth = []\n",
    "for dic in birth_dic:\n",
    "    dfs_birth.append(pd.DataFrame(dic, index=[0]))\n",
    "birth = pd.concat(dfs_birth, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge les infos de naissance avec les infos des artistes\n",
    "artist = artist.merge(birth, left_index=True, right_index=True)\n",
    "artist = artist.set_index(\"Artist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ajoute manuellement les informations pour les artistes importants qui n'ont pas été trouvé sur wikipedia\n",
    "manuel_names = [\n",
    "    (\"sound of legend\", \"MANUEL\", np.NaN, np.NaN, np.NaN, \"français\", np.NaN, np.NaN),\n",
    "    (\"al. hy\", \"MANUEL\", \"15 novembre 1993\", np.NaN, np.NaN, \"français\", np.NaN, \"15 novembre 1993 (Âge: 29 ans), Saint-Amand-les-Eaux\"),\n",
    "    (\"glk\", \"MANUEL\", np.NaN, np.NaN, np.NaN, \"français\", np.NaN, \"Originaire de Bobigny, en Seine-Saint-Denis\"),\n",
    "    (\"paul glaeser\", \"MANUEL\", \"1963\", np.NaN, np.NaN, \"français\", np.NaN, np.NaN),\n",
    "    (\"bolémvn\", \"MANUEL\", \"29 novembre 1996\", np.NaN, np.NaN, \"français\", np.NaN, \"Bryan Mounkala (né le 29 novembre 1996) mieux connu sous le nom de Bolémvn est un rappeur français d'Évry, Essonne, Île-de-France, France.\"),\n",
    "    (\"paul glaeser\", \"MANUEL\", \"1963\", np.NaN, np.NaN, \"français\", np.NaN, np.NaN),\n",
    "    (\"1pliké140\", \"MANUEL\", \"1963\", np.NaN, np.NaN, \"français\", np.NaN, \"1PLIKÉ140 est un jeune rappeur français originaire de Clamart (92) en banlieue parisienne.\"),\n",
    "    (\"jérôme collet\", \"MANUEL\", np.NaN, np.NaN, np.NaN, \"français\", np.NaN, np.NaN),\n",
    "    (\"funnybear\", \"MANUEL\", np.NaN, np.NaN, np.NaN, \"français\", np.NaN, np.NaN),\n",
    "    (\"tom snare\", \"MANUEL\", np.NaN, np.NaN, np.NaN, \"français\", np.NaN, \"Xavier Decanter, mieux connu sous son nom de scène Tom Snare, est un DJ et producteur de disques français originaire de Dunkerque.\"),\n",
    "    (\"funnybear\", \"MANUEL\", \"22 janvier 1991\", np.NaN, np.NaN, \"français\", np.NaN, \"22 janvier 1991 (Âge: 31 ans), Hyères\"),\n",
    "    (\"landy\", \"MANUEL\", \"2000\", np.NaN, np.NaN, \"français\", np.NaN, \"Dylan Sylla Gahoussou, « Landy » de son nom de scène, est né dans le XIXe arrondissement de Paris\"),\n",
    "    (\"dry\", \"MANUEL\", \"19 novembre 1977\", np.NaN, np.NaN, \"français\", np.NaN, \"Landry Delica a grandi à Sevran en Seine-Saint-Denis\"),\n",
    "    (\"sasso\", \"MANUEL\", np.NaN, np.NaN, np.NaN, \"français\", np.NaN, \"Né à Vénissieux d'un père togolais et d'une mère marocaine\"),\n",
    "    (\"neïman\", \"MANUEL\", np.NaN, np.NaN, np.NaN, \"guyanais\", np.NaN, \"NEÏMAN est un chanteur et toaster français de dancehall et de reggae-soul né RAY NEÏMAN en Guyane.\"),\n",
    "    (\"isk\", \"MANUEL\", \"20 mars 2003\", np.NaN, np.NaN, \"français\", np.NaN, \"ISK, de son vrain nom Kais Ben Baccar, est un rappeur français d'origine tunisienne, né au Canada. Habitant La Ferté-sous-Jouarre (77), il est membre du label Bendo 11 Records sous la division rap GrandLine.\"),\n",
    "    (\"yaro\", \"MANUEL\", \"1996\", np.NaN, np.NaN, \"français\", np.NaN, \"Yaro, anciennement Sirsy, est un rappeur français originaire de la ville de Yerres dans l'Essonne.\")\n",
    "]\n",
    "\n",
    "manuel_artist = pd.DataFrame(manuel_names, columns=[\"Artist\", \"Artist_wiki\", \"Naissance\", \"Pays d'origine\", \"Origine\", \"Nationalité\", \"Pays\", \"Summary\"]).set_index(\"Artist\")\n",
    "artist.drop(index=manuel_artist.index, inplace=True)\n",
    "artist = pd.concat([artist, manuel_artist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "artist.to_csv(\"../etapes/3 - scrapping artist data/artist.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanning Wikipédia results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nationalité/Pays/Origine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "artist = pd.read_csv(\"../etapes/3 - scrapping artist data/artist.csv\", encoding=\"utf-8-sig\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_cols = [\"Nationalité\", \"Origine\", \"Pays\", \"Pays d'origine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajout de la colonne \"No data\" pour voir ceux sur qui on n'a pas de données\n",
    "artist[\"No data\"] = (\n",
    "    (artist[\"Naissance\"].isna()) &\n",
    "    (artist[\"Pays d'origine\"].isna()) &\n",
    "    (artist[\"Origine\"].isna()) &\n",
    "    (artist[\"Nationalité\"].isna()) &\n",
    "    (artist[\"Pays\"].isna()) &\n",
    "    (artist[\"Summary\"].isna())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = artist[~artist[\"No data\"]] #On ne garde que les artistes pour lesquels on a des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On supprime les textes dépassant les 40 caractères\n",
    "for col in nationality_cols:\n",
    "    artist.loc[~artist[col].isna(), col] = artist.loc[~artist[col].isna(), col].apply(lambda x: np.NaN if len(x) > 40 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des pays et des gentillé associer\n",
    "gentille_df = pd.read_csv(\"../monde/gentille.csv\")\n",
    "gentille = gentille_df[\"gentille\"].tolist()\n",
    "pays = gentille_df[\"pays\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\AppData\\Local\\Temp\\ipykernel_292\\3502182545.py:4: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  artist.loc[artist[\"Pays\"].str.contains(rgx, regex=True, na=False), \"Pays\"] = p\n",
      "C:\\Users\\quent\\AppData\\Local\\Temp\\ipykernel_292\\3502182545.py:5: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  artist.loc[artist[\"Pays d'origine\"].str.contains(rgx, regex=True, na=False), \"Pays d'origine\"] = p\n"
     ]
    }
   ],
   "source": [
    "#On cherche dans les pays si on trouve quelque chose de connu\n",
    "for p in pays:\n",
    "    rgx = r\"([\\d)()\\], ]|^)\"+ p.lower() + r\"([.,\\[) ]|$)\"\n",
    "    artist.loc[artist[\"Pays\"].str.contains(rgx, regex=True, na=False), \"Pays\"] = p\n",
    "    artist.loc[artist[\"Pays d'origine\"].str.contains(rgx, regex=True, na=False), \"Pays d'origine\"] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cela va permettre de normaliser les noms des pays et des nationalités\n",
    "def cleanning(data, replace_words, p=0.3):\n",
    "    if data is np.NaN:\n",
    "        return np.NaN\n",
    "    \n",
    "    distances = []\n",
    "    for r in replace_words:\n",
    "        distances.append(edit_distance(data.lower(), r.lower())/len(data))\n",
    "    return replace_words[np.argmin(distances)] if np.min(distances) < p else np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Nationalité\"] = artist[\"Nationalité\"].apply(lambda x : cleanning(x, gentille))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Origine\"] = artist[\"Origine\"].apply(lambda x : cleanning(x, pays))\n",
    "artist[\"Pays\"] = artist[\"Pays\"].apply(lambda x : cleanning(x, pays))\n",
    "artist[\"Pays d'origine\"] = artist[\"Pays d'origine\"].apply(lambda x : cleanning(x, pays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nationality(data_to_check, check_list):\n",
    "    if data_to_check is np.NaN:\n",
    "        return np.NaN\n",
    "        \n",
    "    for r in check_list:\n",
    "        regex = r\"([\\d)()\\], ]|^)\"+ r.lower() + r\"([.,\\[) ]|$)\"\n",
    "\n",
    "        if not re.search(regex, data_to_check.lower()) is None:\n",
    "            return r.lower()\n",
    "    return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "nats = artist[~artist[\"Nationalité\"].isna()][\"Nationalité\"].unique()\n",
    "artist.loc[artist[\"Nationalité\"].isna(), \"Nationalité\"] = artist.loc[artist[\"Nationalité\"].isna(), \"Summary\"].apply(lambda x: get_nationality(x, nats))         #Je récupère une première fois avec les nationalités déjà connues\n",
    "artist.loc[artist[\"Nationalité\"].isna(), \"Nationalité\"] = artist.loc[artist[\"Nationalité\"].isna(), \"Summary\"].apply(lambda x: get_nationality(x, gentille))     #Je récupère une deuxième fois avec les gentilles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist.loc[(artist[\"Pays\"].isna()) & (artist[\"Nationalité\"].isna()), \"Pays\"] = artist.loc[(artist[\"Pays\"].isna()) & (artist[\"Nationalité\"].isna()), \"Naissance\"].apply(lambda x: get_nationality(x, pays)) #Je récupère une première fois sur les naissances\n",
    "artist.loc[(artist[\"Pays\"].isna()) & (artist[\"Nationalité\"].isna()), \"Pays\"] = artist.loc[(artist[\"Pays\"].isna()) & (artist[\"Nationalité\"].isna()), \"Summary\"].apply(lambda x: get_nationality(x, pays))   #Je récupère une deuxième fois sur le summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère la nationalité à partir du pays et origine\n",
    "gentille = pd.DataFrame.to_dict(gentille_df.set_index(\"pays\"), orient=\"dict\").get(\"gentille\")\n",
    "\n",
    "for k, v in gentille.items():\n",
    "    artist.loc[(artist[\"Nationalité\"].isna()) & (artist[\"Pays\"] == k), \"Nationalité\"] = v\n",
    "    artist.loc[(artist[\"Nationalité\"].isna()) & (artist[\"Pays d'origine\"] == k), \"Nationalité\"] = v\n",
    "    artist.loc[(artist[\"Nationalité\"].isna()) & (artist[\"Origine\"] == k), \"Nationalité\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "artist.to_csv(\"../etapes/4 - cleanning wikipedia results/nationalité_pays/artist.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commune/Departement/Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "artist = pd.read_csv(\"../etapes/4 - cleanning wikipedia results/nationalité_pays/artist.csv\", encoding=\"utf-8-sig\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des données\n",
    "region = pd.read_csv(\"../france/departements-france.csv\")\n",
    "departement = pd.read_csv(\"../france/departements-france.csv\")\n",
    "commune = pd.read_csv(\"../france/communes-departement-region.csv\")\n",
    "nb_habitant = pd.read_csv(\"../france/nb_habitant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On supprime les colonnes inutiles\n",
    "commune = commune.drop(columns = [\"code_commune_INSEE\", \"nom_commune_postal\", \"code_postal\", \"libelle_acheminement\", \"ligne_5\", \"latitude\", \"longitude\", \"code_commune\", \"article\", \"code_departement\", \"code_region\", \"nom_commune\"])\n",
    "\n",
    "#On rename la colonne nom_commune_complet en nom_commune\n",
    "commune = commune.rename(columns={\"nom_commune_complet\": \"nom_commune\"})\n",
    "\n",
    "#On enleve les arrondisement des villes\n",
    "commune.loc[commune[\"nom_commune\"].str.contains(r\"[A-Za-z]* [0-9]{2}\"), \"nom_commune\"] = commune.loc[commune[\"nom_commune\"].str.contains(r\"[A-Za-z]* [0-9]{2}\"), \"nom_commune\"].str[:-3]\n",
    "\n",
    "#On ajoute le nombre d'habitant au commune\n",
    "commune = commune.merge(nb_habitant[[\"Ville\", \"nb_habitant\"]], left_on=\"nom_commune\", right_on=\"Ville\", how=\"inner\").drop(columns=[\"Ville\"]).dropna().drop_duplicates()\n",
    "\n",
    "#On transforme le nb d'habitant en int\n",
    "commune[\"nb_habitant\"] = commune[\"nb_habitant\"].str.replace(\" \", \"\")\n",
    "commune[\"nb_habitant\"] = commune[\"nb_habitant\"].astype(\"int\")\n",
    "\n",
    "#On transforme tout en lower\n",
    "for col in commune.select_dtypes(\"object\").columns:\n",
    "    commune[col] = commune[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permet de récupérer la localisation d'un artiste à partir de sa naissance ou sommaire wikipedia\n",
    "def get_localisation(row, localisation):\n",
    "    for r in localisation:\n",
    "        regex = r\"([\\d)()\\] ]|^)\" + r.lower() + r\"([.,\\[) ]|$)\"\n",
    "\n",
    "        if (not re.search(regex, str(row[\"Summary\"]).lower()) is None) or (not re.search(regex, str(row[\"Naissance\"]).lower()) is None):\n",
    "            return r.lower()\n",
    "    return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère la région\n",
    "artist[\"Region\"] = artist.apply(lambda x: get_localisation(x, commune[\"nom_region\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère la commune\n",
    "artist[\"Commune\"] = artist.apply(lambda x: get_localisation(x, commune[commune[\"nb_habitant\"] > 2000][\"nom_commune\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère le département\n",
    "artist[\"Departement\"] = artist.apply(lambda x: get_localisation(x, commune[\"nom_departement\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artiste sans localisation\n",
    "artist[\"No localisation\"] = (\n",
    "    (artist[\"Region\"].isna()) &\n",
    "    (artist[\"Commune\"].isna()) &\n",
    "    (artist[\"Departement\"].isna())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les artistes pour qui ont a trouvé une commune/departement/region on leur donne la nationalité française\n",
    "artist.loc[(artist[\"Nationalité\"].isna()) & (~artist[\"No localisation\"]), \"Nationalité\"] = \"français\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permet de récupérer le département à partir de la commune\n",
    "def commune_to_departement(x : str):\n",
    "    value = commune.loc[commune[\"nom_commune\"] == x, \"nom_departement\"].values\n",
    "    if len(value) > 0:\n",
    "        return value[0]\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ajoute les departements pour les communes\n",
    "artist.loc[(~artist[\"Commune\"].isna()) & (artist[\"Departement\"].isna()), \"Departement\"] = artist.loc[(~artist[\"Commune\"].isna()) & (artist[\"Departement\"].isna()), \"Commune\"].apply(commune_to_departement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permet de récupérer la région à partir du département\n",
    "def departement_to_region(x : str):\n",
    "    value = commune.loc[commune[\"nom_departement\"] == x, \"nom_region\"].values\n",
    "    if len(value) > 0:\n",
    "        return value[0]\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ajoute les regions pour les departements\n",
    "artist.loc[(~artist[\"Departement\"].isna()) & (artist[\"Region\"].isna()), \"Region\"] = artist.loc[(~artist[\"Departement\"].isna()) & (artist[\"Region\"].isna()), \"Departement\"].apply(departement_to_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "artist.to_csv(\"../etapes/4 - cleanning wikipedia results/departement_region/artist.csv\", encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2823d83d0138f4e88bebb1ac6893599dadbc35aa0bdabe1fb606adea8faab8b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
