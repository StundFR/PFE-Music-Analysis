{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wikipedia\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(y) for y in np.arange(2004, 2023, 1)]\n",
    "weeks = [int(v) for v in np.linspace(1, 52, 12)]\n",
    "mois = [\"Janvier\", \"Fevrier\", \"Mars\", \"Avril\", \"Mai\", \"Juin\", \"Juillet\", \"Aout\", \"Septembre\", \"Octobre\", \"Novembre\", \"Decembre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:33<00:00,  8.10s/it]\n"
     ]
    }
   ],
   "source": [
    "dfs_chart = {y : {} for y in years}\n",
    "data = []\n",
    "\n",
    "for y in tqdm(years):\n",
    "    for k, w in enumerate(weeks):\n",
    "        url = f\"https://acharts.co/france_singles_top_100/{y}/{w}\"\n",
    "        rq = get(url)\n",
    "        m = mois[k]\n",
    "        if rq.ok:\n",
    "            soup = BeautifulSoup(rq.text)\n",
    "\n",
    "            chart = soup.find(\"table\", {\"id\" : \"ChartTable\"})\n",
    "            trs = chart.findAll(\"tr\")[1:]\n",
    "\n",
    "            for tr in trs:\n",
    "                music_name = tr.find(\"span\", {\"itemprop\" : \"name\"}).text.lower()\n",
    "                rank = tr.find(\"span\", {\"itemprop\" : \"position\"}).text.lower()\n",
    "                artist_name = tr.find(\"span\", {\"itemprop\" : \"byArtist\"}).text[2:-1].lower()\n",
    "                data.append([m, y, str(rank), str(artist_name), str(music_name)])\n",
    "\n",
    "charts = pd.DataFrame(data, columns=[\"Mois\", \"Annee\", \"Rank\", \"Artist\", \"Music\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "charts.to_csv(\"../etapes/1 - scrapping chart/charts.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanning chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    (\"Djadja\", \"Djadja et Dinaz\"),\n",
    "    (\"Lorie\", \"Lorie Pester\"),\n",
    "    (\"-m-\", \"Matthieu Chedid\"),\n",
    "    (\"Priscilla\", \"Priscilla Betti\"),\n",
    "    (\"I Am\", \"IAM\"),\n",
    "    (\"Sofiane\", \"Sofiane Zermani\"),\n",
    "    (\"Justice\", \"Justice (groupe)\"),\n",
    "    (\"Soma Riba\", \"Collectif Métissé\"),\n",
    "    (\"Fresh\", \"Fresh la Peufra\"),\n",
    "    (\"Rosalia\", \"Rosalía\"),\n",
    "    (\"Italo Brothers\", \"ItaloBrothers\"),\n",
    "    (\"Far\\*east Movement\", \"Far East Movement\"),\n",
    "    (\"Odyssey\", \"Odyssey (groupe)\"),\n",
    "    (\"1789\", \"1789 : Les Amants de la Bastille\"),\n",
    "    (\"Clemence\", \"Clémence Saint-Preux\"),\n",
    "    (\"Rose\", \"Rose (chanteuse)\"),\n",
    "    (\"Laeti\", \"Laetitia Kerfa\"),\n",
    "    (\"La Troupe\", \"Mozart, l'opéra rock\"),\n",
    "    (\"Victoria\", \"Victoria Sio\"),\n",
    "    (\"Christine And The Queens\", \"Redcar (artiste)\"),\n",
    "    (\"Earth and Wind And Fire\", \"Earth, Wind And Fire\"),\n",
    "    (\"Jean Roch\", \"Jean-Roch\"),\n",
    "    (\"Keen V\", \"Keen'V\"),\n",
    "    (\"Dinor\", \"Dinor RDT\"),\n",
    "    (\"Koba La D\", \"Koba LaD\"),\n",
    "    (\"scotts\", \"Travis Scott\"),\n",
    "    (\"louane emera\", \"louane\"),\n",
    "    (\"maitre gims\", \"gims\"),\n",
    "    (\"p!nk\", \"pink\"),\n",
    "    (\"Eva\", \"Eva Queen\"),\n",
    "    (\"dj tiësto\", \"tiesto\"),\n",
    "    (\"Karol\", \"Karol G\"),\n",
    "    (\"Khaled\", \"DJ Khaled\"),\n",
    "    (\"Black Eyed Peas\", \"The Black Eyed Peas\"),\n",
    "    (\"L.E.J\", \"LEJ\"),\n",
    "    (\"Disiz la peste\", \"Disiz\"),\n",
    "    (\"k'maro\", \"k. maro\"),\n",
    "    (\"shin sekaï\", \"The Shin Sekaï\"),\n",
    "    (\"the niro\", \"niro\"),\n",
    "    (\"r.i.o.\", \"rio\"),\n",
    "    (\"zayn\", \"zayn malik\"),\n",
    "    (\"dimitri vegas\", \"dimitri vegas & like mike\"),\n",
    "    (\"cauet\", \"Sébastien Cauet\"),\n",
    "    (\"do\", \"the do\"),\n",
    "    (\"les filles\", \"Aurélie Konaté\")\n",
    "]\n",
    "\n",
    "replace_words = [\n",
    "    (\"\\x9c\", \"oe\"),\n",
    "    (\"œ\", \"oe\"),\n",
    "    (\"$\", \"S\"),\n",
    "]\n",
    "\n",
    "charts[\"Artist\"] = charts[\"Artist\"].str.split(\" x \", regex=False).str[0]\n",
    "charts[\"Artist\"] = charts[\"Artist\"].str.split(\" - \", regex=False).str[-1]\n",
    "charts[\"Artist\"] = charts[\"Artist\"].str.split(\" \\+ \", regex=False).str[0]\n",
    "\n",
    "charts[\"Artist\"] = charts[\"Artist\"].apply(lambda x: \"star academy\" if \"star academy\" in x else x)\n",
    "\n",
    "for n1, n2 in names:\n",
    "    charts[\"Artist\"] = charts[\"Artist\"].apply(lambda x: n2.lower() if x.lower() == n1.lower() else x.lower())\n",
    "\n",
    "for n1, n2 in replace_words:\n",
    "    charts[\"Artist\"] = charts[\"Artist\"].str.replace(n1.lower(), n2.lower(), regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "charts.to_csv(\"../etapes/2 - cleanning chart/charts.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping artist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_title_in_wikipedia(title, pourcentage=0.3):\n",
    "    words = [\"(chanteur)\", \"(chanteuse)\", \"(groupe)\", \"(rappeur)\", \"(rappeuse)\", \"(musicien)\", \"(chanteur français)\", \"(france)\", \"(producteur)\", \"(artiste)\", \"(groupe de musique)\"]\n",
    "\n",
    "    wikipedia.set_lang(\"fr\")\n",
    "    results = wikipedia.search(title, results=10)\n",
    "    distance = []\n",
    "    if len(results) > 0:\n",
    "        for element in results:\n",
    "            if any((w in element.lower()) and (edit_distance(element.lower().split(\" (\")[0].strip(), title.lower().strip())/len(title) < pourcentage) for w in words):\n",
    "                return element\n",
    "\n",
    "            distance.append(edit_distance(title.lower().strip(), element.lower().strip()))\n",
    "\n",
    "        return results[np.argmin(distance)] if min(distance)/len(title) < pourcentage else MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_birth(title):\n",
    "    cols = [\"Naissance\", \"Pays d'origine\", \"Origine\", \"Nationalité\", \"Pays\", \"Summary\"]\n",
    "    nats = [\"franco\", \"français\", \"belge\", \"canadien\", \"libanais\", \"réunionnais\"]\n",
    "    dic = {w : MISSING for w in cols}\n",
    "\n",
    "    if title == MISSING:\n",
    "        return dic\n",
    "\n",
    "    url = f\"https://fr.wikipedia.org/wiki/{title}\"\n",
    "    rq = get(url)\n",
    "\n",
    "    if not rq.ok:\n",
    "        return dic\n",
    "    \n",
    "    soup = BeautifulSoup(rq.text)\n",
    "    tables = soup.findAll(\"table\")\n",
    "\n",
    "    for table in tables:\n",
    "        trs = table.findAll(\"tr\")\n",
    "\n",
    "        for tr in trs:\n",
    "            th = tr.find(\"th\")\n",
    "\n",
    "            if th is not None:\n",
    "                for w in cols:\n",
    "                    if w in th.text:\n",
    "                        td = tr.find(\"td\")\n",
    "                        if td is not None:\n",
    "                            dic[w] = td.text.strip().lower()\n",
    "\n",
    "    wikipedia.set_lang(\"fr\")\n",
    "    try:\n",
    "        summary = wikipedia.summary(title, sentences=1)\n",
    "        dic[\"Summary\"] = summary.lower().strip()\n",
    "\n",
    "        if dic[\"Nationalité\"] == MISSING:\n",
    "            for w in nats:\n",
    "                if w in summary:\n",
    "                    dic[\"Nationalité\"] = w.lower().strip()\n",
    "                    return dic\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupére tous les artistes uniques\n",
    "artist = pd.DataFrame(charts[\"Artist\"].unique(), columns=[\"Artist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Artist_wiki\"] = artist[\"Artist\"].apply(lambda x : find_title_in_wikipedia(x, 0.2)) #Trouve les pages wikipedia de chaque artistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "birth_dic = artist[\"Artist_wiki\"].apply(wiki_birth) #Cherche les infos de naissance sur les pages wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforme les infos trouver sur wikipédia en dataframe\n",
    "dfs_birth = []\n",
    "for dic in birth_dic:\n",
    "    dfs_birth.append(pd.DataFrame(dic, index=[0]))\n",
    "birth = pd.concat(dfs_birth, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge les infos de naissance avec les infos des artistes\n",
    "artist = artist.merge(birth, left_index=True, right_index=True)\n",
    "artist = artist.set_index(\"Artist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ajoute manuellement les informations pour les artistes importants qui n'ont pas été trouvé sur wikipedia\n",
    "manuel_names = [\n",
    "    (\"sound of legend\", \"MANUEL\", MISSING, MISSING, MISSING, \"français\", MISSING, MISSING),\n",
    "    (\"al. hy\", \"MANUEL\", \"15 novembre 1993\", MISSING, MISSING, \"français\", MISSING, \"15 novembre 1993 (Âge: 29 ans), Saint-Amand-les-Eaux\"),\n",
    "    (\"glk\", \"MANUEL\", MISSING, MISSING, MISSING, \"français\", MISSING, \"Originaire de Bobigny, en Seine-Saint-Denis\"),\n",
    "    (\"paul glaeser\", \"MANUEL\", \"1963\", MISSING, MISSING, \"français\", MISSING, MISSING),\n",
    "    (\"bolémvn\", \"MANUEL\", \"29 novembre 1996\", MISSING, MISSING, \"français\", MISSING, \"Bryan Mounkala (né le 29 novembre 1996) mieux connu sous le nom de Bolémvn est un rappeur français d'Évry, Essonne, Île-de-France, France.\"),\n",
    "    (\"paul glaeser\", \"MANUEL\", \"1963\", MISSING, MISSING, \"français\", MISSING, MISSING),\n",
    "    (\"1pliké140\", \"MANUEL\", \"1963\", MISSING, MISSING, \"français\", MISSING, \"1PLIKÉ140 est un jeune rappeur français originaire de Clamart (92) en banlieue parisienne.\"),\n",
    "    (\"jérôme collet\", \"MANUEL\", MISSING, MISSING, MISSING, \"français\", MISSING, MISSING),\n",
    "    (\"funnybear\", \"MANUEL\", MISSING, MISSING, MISSING, \"français\", MISSING, MISSING),\n",
    "    (\"tom snare\", \"MANUEL\", MISSING, MISSING, MISSING, \"français\", MISSING, \"Xavier Decanter, mieux connu sous son nom de scène Tom Snare, est un DJ et producteur de disques français originaire de Dunkerque.\"),\n",
    "    (\"funnybear\", \"MANUEL\", \"22 janvier 1991\", MISSING, MISSING, \"français\", MISSING, \"22 janvier 1991 (Âge: 31 ans), Hyères\"),\n",
    "    (\"landy\", \"MANUEL\", \"2000\", MISSING, MISSING, \"français\", MISSING, \"Dylan Sylla Gahoussou, « Landy » de son nom de scène, est né dans le XIXe arrondissement de Paris\"),\n",
    "    (\"dry\", \"MANUEL\", \"19 novembre 1977\", MISSING, MISSING, \"français\", MISSING, \"Landry Delica a grandi à Sevran en Seine-Saint-Denis\"),\n",
    "    (\"sasso\", \"MANUEL\", MISSING, MISSING, MISSING, \"français\", MISSING, \"Né à Vénissieux d'un père togolais et d'une mère marocaine\"),\n",
    "    (\"neïman\", \"MANUEL\", MISSING, MISSING, MISSING, \"guyanais\", MISSING, \"NEÏMAN est un chanteur et toaster français de dancehall et de reggae-soul né RAY NEÏMAN en Guyane.\"),\n",
    "    (\"isk\", \"MANUEL\", \"20 mars 2003\", MISSING, MISSING, \"français\", MISSING, \"ISK, de son vrain nom Kais Ben Baccar, est un rappeur français d'origine tunisienne, né au Canada. Habitant La Ferté-sous-Jouarre (77), il est membre du label Bendo 11 Records sous la division rap GrandLine.\"),\n",
    "    (\"yaro\", \"MANUEL\", \"1996\", MISSING, MISSING, \"français\", MISSING, \"Yaro, anciennement Sirsy, est un rappeur français originaire de la ville de Yerres dans l'Essonne.\")\n",
    "]\n",
    "\n",
    "manuel_artist = pd.DataFrame(manuel_names, columns=[\"Artist\", \"Artist_wiki\", \"Naissance\", \"Pays d'origine\", \"Origine\", \"Nationalité\", \"Pays\", \"Summary\"]).set_index(\"Artist\")\n",
    "artist.drop(index=manuel_artist.index, inplace=True)\n",
    "artist = pd.concat([artist, manuel_artist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "artist.to_csv(\"../etapes/3 - scrapping artist data/artist.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanning Wikipédia results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nationalité/Pays/Origine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gentille_df = pd.read_csv(\"../monde/gentille.csv\")\n",
    "gentille = gentille_df[\"gentille\"].tolist()\n",
    "pays = gentille_df[\"pays\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanning(data, replace_words):\n",
    "    if data == MISSING:\n",
    "        return MISSING\n",
    "    else:\n",
    "        distances = []\n",
    "        for r in replace_words:\n",
    "            distances.append(edit_distance(data.lower(), r.lower()))\n",
    "        return replace_words[np.argmin(distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Nationalité\"] = artist[\"Nationalité\"].apply(lambda x : cleanning(x, gentille))\n",
    "artist[\"Origine\"] = artist[\"Origine\"].apply(lambda x : cleanning(x, gentille))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Pays\"] = artist[\"Pays\"].apply(lambda x : cleanning(x, pays))\n",
    "artist[\"Pays d'origine\"] = artist[\"Pays d'origine\"].apply(lambda x : cleanning(x, pays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajout de la colonne \"No data\" pour voir ceux sur qui on n'a pas de données\n",
    "artist[\"No data\"] = (\n",
    "    (artist[\"Naissance\"] == MISSING) &\n",
    "    (artist[\"Pays d'origine\"] == MISSING) &\n",
    "    (artist[\"Origine\"] == MISSING) &\n",
    "    (artist[\"Nationalité\"] == MISSING) &\n",
    "    (artist[\"Pays\"] == MISSING) &\n",
    "    (artist[\"Summary\"] == MISSING)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = artist[~artist[\"No data\"]] #On ne garde que les artistes pour lesquels on a des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nationality(row, words):\n",
    "    for r in words:\n",
    "        regex = r\"([\\d)()\\] ]|^)\"+ r.lower() + r\"([.,\\[) ]|$)\"\n",
    "\n",
    "        if not re.search(regex, row[\"Summary\"].lower()) is None:\n",
    "            return r.lower()\n",
    "    return MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On utilise le summary pour récupérer la nationalité\n",
    "artist.loc[artist[\"Nationalité\"] == MISSING, \"Nationalité\"] = artist[artist[\"Nationalité\"] == MISSING].apply(lambda x: get_nationality(x, artist[\"Nationalité\"].unique()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pays(row, words):\n",
    "    for r in words:\n",
    "        regex = r\"([\\d)()\\], ]|^)\"+ r.lower() + r\"([.,\\[) ]|$)\"\n",
    "\n",
    "        if not re.search(regex, row[\"Naissance\"].lower()) is None:\n",
    "            return r.lower()\n",
    "    return MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupére une liste de pays\n",
    "gentille_df = pd.read_csv(\"../monde/gentille.csv\")\n",
    "pays = gentille_df[\"pays\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On utilise le summary pour récupérer la Pays\n",
    "artist.loc[artist[\"Pays\"] == MISSING, \"Pays\"] = artist[artist[\"Pays\"] == MISSING].apply(lambda x: get_pays(x, pays), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère la nationalité à partir du pays\n",
    "gentille = pd.DataFrame.to_dict(gentille_df.set_index(\"pays\"), orient=\"dict\").get(\"gentille\")\n",
    "\n",
    "for k, v in gentille.items():\n",
    "    artist.loc[(artist[\"Nationalité\"] == MISSING) & (artist[\"Pays\"] == k), \"Nationalité\"] = v\n",
    "    artist.loc[(artist[\"Nationalité\"] == MISSING) & (artist[\"Pays d'origine\"] == k), \"Nationalité\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les origines sont transposées avec sur la nationalité\n",
    "artist.loc[(artist[\"Nationalité\"] == MISSING) & (artist[\"Origine\"] != MISSING), \"Nationalité\"] = artist.loc[(artist[\"Nationalité\"] == MISSING) & (artist[\"Origine\"] != MISSING), \"Origine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "artist.to_csv(\"../etapes/4 - cleanning wikipedia results/nationalité_pays/artist.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commune/Departement/Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "artist = pd.read_csv(\"../etapes/4 - cleanning wikipedia results/nationalité_pays/artist.csv\")\n",
    "artist.fillna(MISSING, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = pd.read_csv(\"../france/departements-france.csv\")\n",
    "departement = pd.read_csv(\"../france/departements-france.csv\")\n",
    "commune = pd.read_csv(\"../france/communes-departement-region.csv\")\n",
    "nb_habitant = pd.read_csv(\"../france/nb_habitant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On supprime les colonnes inutiles\n",
    "commune = commune.drop(columns = [\"code_commune_INSEE\", \"nom_commune_postal\", \"code_postal\", \"libelle_acheminement\", \"ligne_5\", \"latitude\", \"longitude\", \"code_commune\", \"article\", \"code_departement\", \"code_region\", \"nom_commune\"])\n",
    "\n",
    "#On rename la colonne nom_commune_complet en nom_commune\n",
    "commune = commune.rename(columns={\"nom_commune_complet\": \"nom_commune\"})\n",
    "\n",
    "#On enleve les arrondisement des villes\n",
    "commune.loc[commune[\"nom_commune\"].str.contains(r\"[A-Za-z]* [0-9]{2}\"), \"nom_commune\"] = commune.loc[commune[\"nom_commune\"].str.contains(r\"[A-Za-z]* [0-9]{2}\"), \"nom_commune\"].str[:-3]\n",
    "\n",
    "#On ajoute le nombre d'habitant au commune\n",
    "commune = commune.merge(nb_habitant[[\"Ville\", \"nb_habitant\"]], left_on=\"nom_commune\", right_on=\"Ville\", how=\"inner\").drop(columns=[\"Ville\"]).dropna().drop_duplicates()\n",
    "\n",
    "#On transforme le nb d'habitant en int\n",
    "commune[\"nb_habitant\"] = commune[\"nb_habitant\"].str.replace(\" \", \"\")\n",
    "commune[\"nb_habitant\"] = commune[\"nb_habitant\"].astype(\"int\")\n",
    "\n",
    "#On transforme tout en lower\n",
    "for col in commune.select_dtypes(\"object\").columns:\n",
    "    commune[col] = commune[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_localisation(row, localisation):\n",
    "    for r in localisation:\n",
    "        regex = r\"([\\d)()\\] ]|^)\"+ r.lower() + r\"([.,\\[) ]|$)\"\n",
    "\n",
    "        if (not re.search(regex, row[\"Summary\"].lower()) is None) or (not re.search(regex, row[\"Naissance\"].lower()) is None):\n",
    "            return r.lower()\n",
    "    return MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Region\"] = artist.apply(lambda x: get_localisation(x, region[\"nom_region\"].unique()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Commune\"] = artist.apply(lambda x: get_localisation(x, commune[commune[\"nb_habitant\"] > 2000][\"nom_commune\"].unique()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"Departement\"] = artist.apply(lambda x: get_localisation(x, departement[\"nom_departement\"].unique()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[\"No localisation\"] = (\n",
    "    (artist[\"Region\"] == MISSING) &\n",
    "    (artist[\"Commune\"] == MISSING) &\n",
    "    (artist[\"Departement\"] == MISSING)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les artistes pour qui ont a trouvé une commune/departement/region on leur donne la nationalité française\n",
    "artist.loc[(artist[\"Nationalité\"] == MISSING) & (~artist[\"No localisation\"]), \"Nationalité\"] = \"français\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commune_to_departement(x : str):\n",
    "    value = commune.loc[commune[\"nom_commune\"] == x, \"nom_departement\"].values\n",
    "    if len(value) > 0:\n",
    "        return value[0]\n",
    "    else:\n",
    "        return MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ajoute les departements pour les communes\n",
    "artist.loc[(artist[\"Commune\"] != MISSING) & (artist[\"Departement\"] == MISSING), \"Departement\"] = artist.loc[(artist[\"Commune\"] != MISSING) & (artist[\"Departement\"] == MISSING), \"Commune\"].apply(commune_to_departement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def departement_to_region(x : str):\n",
    "    value = commune.loc[commune[\"nom_departement\"] == x, \"nom_region\"].values\n",
    "    if len(value) > 0:\n",
    "        return value[0]\n",
    "    else:\n",
    "        return MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ajoute les regions pour les departements\n",
    "artist.loc[(artist[\"Departement\"] != MISSING) & (artist[\"Region\"] == MISSING), \"Region\"] = artist.loc[(artist[\"Departement\"] != MISSING) & (artist[\"Region\"] == MISSING), \"Departement\"].apply(departement_to_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "artist.to_csv(\"../etapes/4 - cleanning wikipedia results/departement_region/artist.csv\", encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2823d83d0138f4e88bebb1ac6893599dadbc35aa0bdabe1fb606adea8faab8b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
